# Multi-map Saliency-driven Gaze Behavior for Virtual Characters

Unreal Engine project of [Multi-map Saliency-driven Gaze Behavior for Virtual Characters](https://hal.science/hal-03796523v2). <br/>
In this work, we propose a general method to synthesize gaze behaviors for interactive characters by combining models of *visual saliency*, *saccadic movements*, and *eye-head animation*, while enabling designers to finely control customizable features.

> "Multi-map Saliency-driven Gaze Behavior for Virtual Characters; I. Goudé, A. Bruckert, A.H. Olivier, J.Pettré, R. Cozot, K. Bouatouch, M. Christie, L. Hoyet; in submission process; https://doi.org/10.1109/TVCG.2023.3244679"

## Requirements

Unreal Engine $\ge$ 5.0.3 <br/>
Visual Studio  $\ge$ 2019 + [plugins](https://docs.unrealengine.com/4.26/en-US/ProductionPipelines/DevelopmentSetup/VisualStudioSetup/) <br/>
.NET Core 3.1 SDK <br/>
Libtorch $\ge$ 1.9 <br/>
OpenCV $\ge$ 4.5 <br/>
Cuda $\ge$ 10.2 <br/>
cuDNN (compatible with cuda version)

> <strong>⚠️WARNING: With Unreal Engine 5.0.3, a deadlock happens when loading opencv_world453.dll</strong> <br/>
A fix exists but requires to build Unreal from source and to bytepatch a function, please visit: <br/>
[the Unreal forum with the fix explained](https://forums.unrealengine.com/t/fplatformprocess-getdllhandle-causes-deadlock-when-loading-opencv-world/571735), and [the github of the guy if you need more details](https://github.com/wongfei/ue4-mediapipe-plugin/commit/c502349e4c0c8960cebbbd294397a157c38e00a1)

## Results

<html>
    <body>
        <p align="center">
            <img src="Docs/images/teaser.png" alt="Teasure">
        </p>
    </body>
</html>
The figure shows a sequence of gaze and head motions automatically generated by our method (top row) through the identification and selection of salient features in the character's field of view (blue circle in the bottom row).

## Model

<html>
    <body>
        <p align="center">
            <img src="Docs/images/framework.png" alt="Flowchart">
        </p>
    </body>

The flowchart of our method describing the five-step process:
<ol>
    <li> Rendering the image in the character's field of view </li>
    <li> The image of the scene is passed through a visual saliency model, which outputs an eye-fixation probability for each pixel </li>
    <li> The predicted saliency map is combined with several human oculomotor biases, and merged into a fixation distribution </li>
    <li> The position and the duration of the fixation are stochastically determined </li>
    <li> Given the character's current eye and head orientations, its gaze is animated towards the new fixation point. When the duration of the fixation is reached, the process is reiterated from step 1 </li>
</ol>
</html>

## Contact

> Ific Goudé <br />
Research Scientist in Computer Graphics <br />
Email: goude.ific@gmail.com <br />
Website: [https://igoude.github.io/](https://igoude.github.io/)
